The pre-trained BERT model contained in this directory was downloaded from:

* https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip


You can read more about this model here:

* https://github.com/google-research/bert


Please note that the original authors released this pre-trained BERT model under the Apache-2.0 license. This model's original LICENSE is available for review here:

* https://github.com/google-research/bert/blob/master/LICENSE


For clarity of attribution and for additional information, here is the citation for the authors' original article referencing this pre-trained model:

* Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. (2019). "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models".`arXiv preprint arXiv:1908.08962v2 <https://arxiv.org/abs/1908.08962>`_.

And, more information regarding BERT itself can be found in the original paper:

- Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. (2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". `arXiv preprint arXiv:1810.04805 <https://arxiv.org/abs/1810.04805>`_.