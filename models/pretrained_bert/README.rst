The pre-trained BERT model contained in this directory was downloaded from:

* https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip


You can read more about this model here:

* https://github.com/google-research/bert


Please note that the original authors released this pre-trained BERT model under the Apache-2.0 license. This model's original LICENSE is available for review here:

* https://github.com/google-research/bert/blob/master/LICENSE


For clarity of attribution and for additional information, here is the citation for the authors' original article referencing this model:

* Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. (2019). "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models".[arXiv preprint arXiv:1908.08962v2](https://arxiv.org/abs/1908.08962)